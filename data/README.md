# Data Directory

This directory contains the data files used and generated by the Generic AI Audit Agent.

## Structure

```
data/
├── raw/           # Raw data from the crawler
├── processed/     # Processed data from the extractor
├── analyzed/      # Analysis results from the analyzer
├── reports/       # Generated reports from the reporter
└── README.md      # This documentation file
```

## Data Flow

The data flows through the system in the following order:

1. **Raw Data** (`raw/`): The crawler component saves raw data in JSON format in this directory. Each file contains the raw output from a crawling session, including all the pages visited, their content, and any extracted information.

2. **Processed Data** (`processed/`): The extractor component reads the raw data, processes it to extract structured information, and saves the results in this directory. The processed data is also in JSON format but has a more structured and cleaned format.

3. **Analysis Results** (`analyzed/`): The analyzer component reads the processed data, performs various analyses (readability, sentiment, SEO, etc.), and saves the results in this directory. The analysis results include metrics, scores, and insights derived from the data.

4. **Reports** (`reports/`): The reporter component reads the analysis results and generates reports in various formats (text, HTML, CSV) in this directory. These reports are the final output of the system and can be shared with users.

## File Naming Convention

Files in these directories follow a naming convention that includes:

- The domain name (e.g., `example_com`)
- A timestamp (e.g., `20250512_123456`)
- The appropriate file extension (`.json`, `.txt`, `.html`, `.csv`)

For example:

- Raw data: `example_com_20250512_123456.json`
- Processed data: `processed_example_com_20250512_123456.json`
- Analysis results: `analyzed_example_com_20250512_123456.json`
- Reports:
  - `example_com_20250512_123456.txt`
  - `example_com_20250512_123456.html`
  - `example_com_20250512_123456.csv`

## Data Retention

By default, the system retains all data files indefinitely. If you want to implement a data retention policy, you can:

1. Manually delete older files that are no longer needed
2. Implement an automated cleanup script that removes files older than a certain age
3. Configure the system to automatically delete intermediate files after the final reports are generated

## Storage Requirements

The storage requirements depend on the size and complexity of the websites being analyzed. As a rough guideline:

- Raw data: 1-10 MB per website (depends on the number of pages crawled)
- Processed data: 0.5-5 MB per website
- Analysis results: 0.5-5 MB per website
- Reports: 0.1-2 MB per website (depends on the report formats)

For large-scale operations, consider implementing a database backend instead of file-based storage.

## Backup Recommendations

It's recommended to regularly back up the `reports` directory, as it contains the final output of the system. The other directories contain intermediate data that can be regenerated if needed.

If you're using the system for critical applications, consider implementing a comprehensive backup strategy for all data directories.
